{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snxSUWXD7q_p"
   },
   "source": [
    "## Домашнее задание по неделе 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-8OLsAV7wrQ"
   },
   "source": [
    "Как было рассказано на лекции, стохастический градиентый спуск сходится быстрее, чем полный, хотя и менее стабильно. В этом задании вам предлагается реализовать стохастический градиентный спуск и сравнить его с точным вычислением весов линейной модели по скорости работы и значению метрики качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgQyWw5o7Nej"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGD1wQgMruJw"
   },
   "source": [
    "### Задание 0\n",
    "\n",
    "Реализуйте класс ```LinearRegressionSGD``` c обучением и и применением линейной регрессии, построенной с помощью стохастического градиентного спуска, с заданным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearRegression(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=1000, w0=None, alpha=1e-2):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(d)\n",
    "\n",
    "        self.w = self.w0\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            self.w_history.append(self.w)\n",
    "\n",
    "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
    "\n",
    "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
    "                break\n",
    "          \n",
    "            self.w = w_new\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        l, d = X.shape\n",
    "\n",
    "        y_pred = []\n",
    "\n",
    "        for i in range(l):\n",
    "            y_pred.append(np.dot(X[i], self.w))\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        \n",
    "        l, d = X.shape\n",
    "        gradient = []\n",
    "        \n",
    "        for j in range(d):\n",
    "            dQ = 0\n",
    "            for i in range(l):\n",
    "                dQ += (2/l) * X[i][j] * (np.dot(X[i], self.w) - y[i])\n",
    "            gradient.append(dQ)\n",
    "\n",
    "        return np.array(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZxdV27R9-uc"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "class LinearRegressionSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=100, w0=None, alpha=1e-4):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "        \n",
    "        X = X[:1, :]\n",
    "\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(d)\n",
    "\n",
    "        self.w = self.w0\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            self.w_history.append(self.w)\n",
    "\n",
    "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
    "\n",
    "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
    "                break\n",
    "          \n",
    "            self.w = w_new\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        l, d = X.shape\n",
    "\n",
    "        y_pred = []\n",
    "\n",
    "        for i in range(l):\n",
    "            y_pred.append(np.dot(X[i], self.w))\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "        gradient = []\n",
    "        \n",
    "        for j in range(d):\n",
    "            dQ = 0\n",
    "            for i in range(l):\n",
    "                dQ += (2/l) * X[i][j] * (np.dot(X[i], self.w) - y[i])\n",
    "            gradient.append(dQ)\n",
    "\n",
    "        return np.array(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNOm9-bXpdT3"
   },
   "source": [
    "Проверять работу мы будем на имеющемся в sklearn наборе данных boston: в нём нужно по информации о доме предсказать его стоимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c24JCwes9-pe"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eIJwWnInXnr"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Метрикой качества в нашей задаче будет MAPE - Mean Absolute Percentage Error. Реализуйте её с заданным интефейсом и вычислите \n",
    "```MAPE(y_test, y_0)```, где ```y_0 = (mean(y_test), mean(y_test), ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znoDavxyuLsi"
   },
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        y_true: np.array (l)\n",
    "        y_pred: np.array (l)\n",
    "        ---\n",
    "        output: float [0, +inf)\n",
    "    \"\"\"\n",
    "    #from sklearn.utils import check_arrays не импортирует чего-то\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        #y_true, y_pred = check_arrays(y_true, y_pred)\n",
    "\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    #if _is_1d(y_true): \n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return mean_absolute_percentage_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6mTAykeojwp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947 23.84078947\n",
      " 23.84078947 23.84078947]\n",
      "[28.4 31.1 23.5 26.6 19.6 14.3 50.  14.3 20.7 37.6 20.4 27.5 36.2 32.\n",
      " 33.1 48.8 24.6 26.4 23.2 17.  41.3 14.9 18.5 25.  36.4 19.5 27.1 14.9\n",
      " 46.  17.9 30.3 31.6 23.1 24.7 16.7 18.3  8.4 37.3 22.1 22.  46.7 30.1\n",
      " 12.1 29.1 16.6 23.9 19.9 21.4 45.4 15.6 22.7 12.5 24.3 43.8 22.  33.8\n",
      " 19.3 22.6 16.1 15.  19.6 21.2 50.  50.  29.4 17.8 22.8  8.8 32.5 42.8\n",
      " 12.6 28.6 19.1 50.  27.5 23.7 50.   7.2 18.7 37.  22.9 22.9 17.1 22.\n",
      " 23.6 23.9 27.1 29.  22.2  7.  20.7 18.5 21.6 23.  16.  15.  23.9 24.4\n",
      " 22.6 19.8 22.2 18.6 19.7 23.1 13.5 21.2 23.1 13.6 22.8 18.2 13.1 23.2\n",
      " 22.8 25.1 18.9 10.9 19.3 17.4 15.6 20.6 50.  32.7 21.8 13.4 16.6 23.6\n",
      " 11.  23.8 23.1 33.2 28.2  8.5 32.4 29.6 17.1 24.2 26.4 33.2 10.5  8.8\n",
      " 28.  10.5 15.4 15.3 10.4 15.7 43.1 24.7 21.  19.4 10.9 21.7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.41588297684096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0 = np.array([np.mean(y_test) for i in y_test])\n",
    "print(y_0)\n",
    "print(y_test)\n",
    "MAPE(y_test, y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nNy2ITxuMKf"
   },
   "source": [
    "### Задание 2 \n",
    "\n",
    "Обучите ```LinearRegressionSGD``` с базовыми параметрами на тренировочном наборе данных (```X_train```, ```y_train```), сделайте предсказание на тестовых данных ```X_test```, записав результат в переменную ```y_pred_sgd```  и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BIHwAwUvB-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.60114989e+168 -5.80636786e+168 -6.35788461e+168 -6.30722568e+168\n",
      " -6.42215043e+168 -5.81757784e+168 -6.73212290e+168 -7.20526483e+168\n",
      " -5.59505765e+168 -6.18647702e+168 -6.72492631e+168 -6.20022008e+168\n",
      " -5.44214525e+168 -5.82911161e+168 -5.76566559e+168 -5.88975549e+168\n",
      " -5.97387782e+168 -5.43639883e+168 -6.09290107e+168 -6.20900607e+168\n",
      " -6.63492544e+168 -8.01260655e+168 -7.06576615e+168 -6.05477672e+168\n",
      " -6.73394683e+168 -5.62890737e+168 -6.32843504e+168 -7.53008797e+168\n",
      " -5.37853764e+168 -5.12814363e+168 -5.91108884e+168 -6.02097149e+168\n",
      " -6.43077816e+168 -5.58971754e+168 -8.78953350e+168 -6.83713262e+168\n",
      " -5.50688534e+168 -5.73840991e+168 -6.19667197e+168 -5.96658270e+168\n",
      " -5.95467644e+168 -6.27096953e+168 -8.60086523e+168 -5.65657216e+168\n",
      " -5.72498167e+168 -6.66236755e+168 -8.32853138e+168 -8.64923823e+168\n",
      " -5.50937207e+168 -6.27319146e+168 -6.94163912e+168 -8.80103705e+168\n",
      " -6.06144554e+168 -5.93713581e+168 -6.01779992e+168 -5.93072139e+168\n",
      " -7.05725568e+168 -8.71057042e+168 -5.70382190e+168 -8.80424129e+168\n",
      " -7.21053211e+168 -7.08916152e+168 -6.87865301e+168 -5.90928677e+168\n",
      " -6.04275482e+168 -7.21148232e+168 -6.05812947e+168 -7.68173212e+168\n",
      " -5.50582054e+168 -6.29337236e+168 -8.71808658e+168 -6.05042051e+168\n",
      " -8.72744042e+168 -8.29722305e+168 -6.78517370e+168 -5.44931225e+168\n",
      " -6.12623395e+168 -8.80605463e+168 -6.06685556e+168 -6.59681949e+168\n",
      " -6.25815382e+168 -6.39661824e+168 -6.12097262e+168 -6.33484872e+168\n",
      " -6.02370283e+168 -6.05549069e+168 -6.04811773e+168 -6.23540276e+168\n",
      " -5.54105069e+168 -5.72340567e+168 -5.53882843e+168 -6.06301806e+168\n",
      " -5.81545300e+168 -6.03336969e+168 -5.94876319e+168 -6.28139394e+168\n",
      " -6.05808401e+168 -5.86630454e+168 -6.13392040e+168 -6.82313672e+168\n",
      " -5.92740584e+168 -3.55553850e+168 -5.74549044e+168 -6.15223481e+168\n",
      " -4.81848156e+168 -6.74847029e+168 -6.20901692e+168 -9.05089551e+168\n",
      " -6.25356399e+168 -6.20699525e+168 -8.80190161e+168 -8.72508494e+168\n",
      " -7.12939978e+168 -6.17850311e+168 -6.05282642e+168 -8.80720042e+168\n",
      " -6.06283489e+168 -6.45212443e+168 -4.69891568e+168 -8.63143292e+168\n",
      " -8.75374330e+168 -6.02774224e+168 -6.80505541e+168 -6.97667543e+168\n",
      " -6.09122391e+168 -6.18616376e+168 -5.76468653e+168 -5.24224030e+168\n",
      " -6.20794268e+168 -6.02006748e+168 -5.65980463e+168 -8.10697996e+168\n",
      " -5.81017303e+168 -6.18192044e+168 -8.69397673e+168 -5.98421621e+168\n",
      " -6.17118342e+168 -5.76492663e+168 -8.80932474e+168 -5.00372069e+168\n",
      " -5.99993240e+168 -8.73986956e+168 -8.70697889e+168 -6.41576889e+168\n",
      " -6.09250668e+168 -5.09910203e+168 -5.90078811e+168 -6.22585576e+168\n",
      " -6.00162748e+168 -6.38384559e+168 -4.98593003e+168 -6.15459749e+168]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.296541086556417e+169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = LinearRegressionSGD().fit(X_train, y_train)\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "print(y_pred_sgd)\n",
    "MAPE(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.79905837e+13 2.72725754e+13 2.79955170e+13 4.50611862e+13\n",
      " 9.15869584e+13 9.03920144e+13 8.61998939e+13 9.83532513e+13\n",
      " 5.75803054e+13 7.31651269e+13 8.37618222e+13 7.40857401e+13\n",
      " 5.96313925e+13 4.32600886e+13 3.94890017e+13 6.47115733e+13\n",
      " 5.37434460e+13 6.86628915e+13 6.46408406e+13 8.97416265e+13\n",
      " 8.85716643e+13 1.08732064e+14 8.71279638e+13 6.12079207e+13\n",
      " 4.68070929e+13 5.87112911e+13 8.03817743e+13 1.00979917e+14\n",
      " 4.74987471e+13 9.27611289e+13 3.08040229e+13 7.18184989e+13\n",
      " 3.01675655e+13 4.85747810e+13 1.14323865e+14 8.20298561e+13\n",
      " 9.34223649e+13 2.28865063e+13 6.70901824e+13 6.82232124e+13\n",
      " 5.41562576e+13 7.44105411e+13 1.13955118e+14 2.82483347e+13\n",
      " 7.45340957e+13 5.10204214e+13 1.08844483e+14 1.06902855e+14\n",
      " 5.24467519e+13 8.12906728e+13 9.22276536e+13 1.15030276e+14\n",
      " 7.15194692e+13 5.61964659e+13 7.46253818e+13 6.43022640e+13\n",
      " 8.36441000e+13 1.08341397e+14 6.22292228e+13 1.08947204e+14\n",
      " 9.63830067e+13 8.49938828e+13 8.85658099e+13 6.32478940e+13\n",
      " 5.85582549e+13 9.92534278e+13 5.48692447e+13 1.06883986e+14\n",
      " 6.69597611e+13 4.60061100e+13 1.12244339e+14 6.26609815e+13\n",
      " 1.08656811e+14 1.04864071e+14 8.07592943e+13 6.04569800e+13\n",
      " 7.22823072e+13 1.17509748e+14 6.10190342e+13 4.55790574e+13\n",
      " 6.95574391e+13 4.83533069e+13 4.89141642e+13 5.37444903e+13\n",
      " 7.05482219e+13 7.49512823e+13 6.82534697e+13 2.91674482e+13\n",
      " 5.41777860e+13 9.04556692e+13 5.47179874e+13 7.25343894e+13\n",
      " 6.96772216e+13 5.82430215e+13 6.76169178e+13 8.00705296e+13\n",
      " 5.90752953e+13 6.97796790e+13 7.02552229e+13 8.66794156e+13\n",
      " 5.48092366e+13 5.96263933e+13 5.46077198e+13 9.29224115e+13\n",
      " 7.30114332e+13 7.79407191e+13 7.80172427e+13 1.22792255e+14\n",
      " 7.44806232e+13 2.74107604e+13 1.11834249e+14 1.07682383e+14\n",
      " 8.67192329e+13 7.35516800e+13 6.93023617e+13 1.13703373e+14\n",
      " 8.26993964e+13 6.28472705e+13 8.04456187e+13 9.99661313e+13\n",
      " 1.07927688e+14 4.82222075e+13 7.67151252e+13 1.00758736e+14\n",
      " 7.84415027e+13 7.42004224e+13 8.92653842e+13 7.71854759e+13\n",
      " 7.27610731e+13 6.45241368e+13 5.08955961e+13 1.00729825e+14\n",
      " 4.09038214e+13 4.51437890e+13 1.13540753e+14 5.44659688e+13\n",
      " 4.66008086e+13 4.56478119e+13 1.14925645e+14 7.02379443e+13\n",
      " 4.83883710e+13 1.10331008e+14 1.13982734e+14 8.82235408e+13\n",
      " 9.14544809e+13 8.27344525e+13 6.49928880e+13 2.80510911e+13\n",
      " 6.17436838e+13 3.40202059e+13 7.22784062e+13 6.31532490e+13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "391769752150773.2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDRegressor().fit(X_train, y_train)\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "print(y_pred_sgd)\n",
    "MAPE(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWappMdMtIPV"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Вычислите веса по точной формуле, используя ```X_train``` и ```y_train```; предскажите с их помощью целевую переменную на ```X_test```, записав результат в переменную ```y_pred_lr``` и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjMUlPje9-k0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.391972849542015"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y_pred_lr = LinearRegression().fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "MAPE(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yL9L-4cwxZho"
   },
   "source": [
    "## Бонусное задание по неделе 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZFaUn7yx04u"
   },
   "source": [
    "До этого вы релизовывали модели, в которых не было штрафа за величину весов ```w```. Как было рассказано ранее в лекциях, это может привести к неустойчивости модели и переобучению. Чтобы избежать этих эффектов, предлагается добавить к оптимизируемому функционалу L2-норму весов; таким образом, будем решать задачу гребневой регрессии, Ridge:\n",
    "\n",
    "$$ \\frac{1}{l}(Xw-y)^T(Xw-y) +\\gamma||w||_2 \\rightarrow \\min_{w}. $$\n",
    "\n",
    "### Задание 11\n",
    "Реализуйте обучение такой модели в матричном виде (смотрите дополнительные материалы к этой неделе) с помощью стохастического градиентного спуска. Класс должен совпадать по набору реализованных функций с ```LinearRegressionVectorized```, разница будет лишь в параметре $\\gamma$, отвечающем за степень регуляризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEXqBqmGxWDz"
   },
   "outputs": [],
   "source": [
    "class RidgeSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=1000, w0=None, alpha=1e-2, gamma=0):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        gamma: коэффициент регуляризации\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t9rqXFu8Pq6"
   },
   "source": [
    "Так же, как и в основном задании, обучите модель с базовыми параметрами на тренировочных данных и сделайте прогноз y_pred_ridge. Выведите значение MAPE(y_test, y_pred_ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6A2hak_A8QPO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW04-sgd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
