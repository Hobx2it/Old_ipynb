{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <left>Бронников Максим Андреевич</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max120199@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   <center>Домашняя работа №6-7</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импортируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"mai-ml-hw-models/Train.csv\")\n",
    "test_df = pd.read_csv(\"mai-ml-hw-models/Test.csv\")\n",
    "sample_df = pd.read_csv(\"mai-ml-hw-models/SampleSubmission.csv\")\n",
    "\n",
    "indexes = test_df.id\n",
    "\n",
    "test_df = test_df.drop(['id'], axis = 1)\n",
    "train_df = train_df.drop(['id'], axis = 1)\n",
    "train_df[train_df.floor > 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на зависимости между данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = plt.figure(figsize= (14, 6))\n",
    "ax1 = fi.add_subplot()\n",
    "#ax1.set_title('Матрица корелляции')\n",
    "sns.heatmap(train_df[train_df.build_tech.notna()].corr(), annot=True, cmap = \"coolwarm\", linewidth = 0.1, ax = ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.plot.scatter(x = 'area', y = 'price', figsize = (14, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уберем очеидные выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.price > 60000000].plot.scatter(x = 'area', y = 'price', figsize = (15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.price.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.price < 550000].plot.scatter(x = 'area', y = 'price', figsize = (15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.price < 60000000] \n",
    "train_df = train_df[train_df.price > 550000]\n",
    "#train_df = train_df[train_df.price > ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.price.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код для генерации фичи - количества ключевых слов\n",
    "\n",
    "Было решено не использовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_feature = train_df[['kw1', 'kw2', 'kw3', 'kw4', 'kw5', 'kw6', \\\n",
    "#                         'kw7', 'kw8', 'kw9', 'kw10', 'kw11', 'kw12', 'kw13']].sum(axis = 1)\n",
    "# train_df['n_kw'] = new_feature\n",
    "\n",
    "# new_feature = test_df[['kw1', 'kw2', 'kw3', 'kw4', 'kw5', 'kw6', \\\n",
    "#                         'kw7', 'kw8', 'kw9', 'kw10', 'kw11', 'kw12', 'kw13']].sum(axis = 1)\n",
    "# test_df['n_kw'] = new_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсим дату и создаем новые фичи\n",
    "\n",
    "Достаем месяц и год, однако я решил, что информация о годе избыточна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seria = train_df.date.str.split(pat = '-')\n",
    "type(seria)\n",
    "#train_df['year'] = seria.str[0]\n",
    "train_df['mounth'] = seria.str[1]\n",
    "# dataset['Gate'] = seria.str[0]\n",
    "# dataset['Numeric'] = seria.str[1::]\n",
    "\n",
    "seria = test_df.date.str.split(pat = '-')\n",
    "type(seria)\n",
    "# test_df['year'] = seria.str[0]\n",
    "test_df['mounth'] = seria.str[1]\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание фичи проверяющей есть ли несфотографированные комнаты\n",
    "\n",
    "На валидации не было прироста, поэтому не используется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_feature = train_df.n_photos.copy()\n",
    "# new_feature[train_df.n_photos < train_df.rooms] = 0\n",
    "# new_feature[train_df.n_photos >= train_df.rooms] = 1\n",
    "# train_df['rooms_with_photo'] = new_feature\n",
    "\n",
    "# new_feature = test_df.n_photos.copy()\n",
    "# new_feature[test_df.n_photos < test_df.rooms] = 0\n",
    "# new_feature[test_df.n_photos >= test_df.rooms] = 1\n",
    "# test_df['rooms_with_photo'] = new_feature\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем новую фичу, объединяющую данные о районе и улице\n",
    "\n",
    "А также определяем столбцы для кодирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_cols = ['date', 'mounth', 'place']\n",
    "\n",
    "test_df['place'] = 1000 * test_df.area + test_df.street_id\n",
    "train_df['place'] = 1000 * train_df.area + train_df.street_id\n",
    "\n",
    "train2_df = train_df.copy()\n",
    "test2_df = test_df.copy()\n",
    "train2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train2_df, test2_df], axis = 0)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for col in OH_cols:\n",
    "    print(f\"Encoding {col}\")\n",
    "    full_df[col] = encoder.fit_transform(full_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполняем пропуски в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = full_df.g_lift.isnull()\n",
    "msk2 = full_df.floor > 5\n",
    "msk3 = full_df.g_lift.notnull()\n",
    "full_df.g_lift[msk] = 0\n",
    "full_df.g_lift[msk] = full_df.g_lift[msk3].mean()\n",
    "\n",
    "msk = full_df.metro_dist.isnull()\n",
    "msk3 = full_df.metro_dist.notnull()\n",
    "new_feature = full_df.metro_dist.copy()\n",
    "# new_feature[msk] = 1\n",
    "# new_feature[msk3] = 0\n",
    "full_df.metro_dist[msk] = full_df.metro_dist[msk3].mean()\n",
    "full_df['not_metro'] = new_feature\n",
    "# full_df.head(20)\n",
    "\n",
    "full_df.build_tech[full_df.balcon != 0] = full_df.balcon[full_df.balcon != 0]\n",
    "full_df.build_tech[full_df.build_tech.isnull()] = \\\n",
    "full_df.build_tech[full_df.balcon == 0 & full_df.build_tech.notnull()].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем статисткики - фичи\n",
    "Не все статистики используются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#full_df['mean_md'] = full_df['area'].map(full_df.groupby('area')['metro_dist'].mean())\n",
    "\n",
    "# full_df['max_area_p'] = train2_df.price.max()\n",
    "# full_df['min_area_p'] = train2_df.price.min()\n",
    "full_df['mean_area_p'] = train2_df.price.mean()\n",
    "\n",
    "#full_df.max_area_p[full_df.area < 214] = full_df.area[full_df.area < 214].map(train2_df.groupby('area')['price'].max())\n",
    "#full_df.min_area_p[full_df.area < 214] = full_df.area[full_df.area < 214].map(train2_df.groupby('area')['price'].min())\n",
    "full_df.mean_area_p[full_df.area < 214] = full_df.area[full_df.area < 214].map(train2_df.groupby('area')['price'].mean())\n",
    "\n",
    "\n",
    "#full_df['max_rooms_p'] = full_df['rooms'].map(train2_df.groupby('rooms')['price'].max())\n",
    "#full_df['min_rooms_p'] = full_df['rooms'].map(train2_df.groupby('rooms')['price'].min())\n",
    "full_df['mean_rooms_p'] = full_df['rooms'].map(train2_df.groupby('rooms')['price'].mean())\n",
    "\n",
    "#full_df['mean_balcon_p'] = full_df['balcon'].map(train2_df.groupby('balcon')['price'].mean())\n",
    "\n",
    "#full_df['mean_photo_p'] = full_df['n_photos'].map(train2_df.groupby('n_photos')['price'].mean())\n",
    "\n",
    "\n",
    "full_df['mean_floor_p'] = full_df['floor'].map(train2_df.groupby('floor')['price'].mean())\n",
    "\n",
    "\n",
    "train2_df = full_df[full_df.price.notnull()]\n",
    "\n",
    "#full_df['mean_mounth_p'] = full_df['mounth'].map(train2_df.groupby('mounth')['price'].mean())\n",
    "#full_df['max_mounth_p'] = full_df['mounth'].map(train2_df.groupby('mounth')['price'].max())\n",
    "#full_df['min_mounth_p'] = full_df['mounth'].map(train2_df.groupby('mounth')['price'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем обновленные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_df = full_df[full_df.price.notnull()]\n",
    "\n",
    "test2_df = full_df[full_df.price.isnull()]\n",
    "test2_df.drop(\"price\", axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "train2_df.drop(\"g_lift\", axis = 1, inplace = True)\n",
    "test2_df.drop(\"g_lift\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем целевой столбец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = train2_df.price\n",
    "train2_df.drop(['price'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = plt.figure(figsize= (18, 8))\n",
    "ax1 = fi.add_subplot()\n",
    "#ax1.set_title('Матрица корелляции')\n",
    "sns.heatmap(train2_df.corr(), annot=True, cmap = \"coolwarm\", linewidth = 0.1, ax = ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALISATION\n",
    "\n",
    "train2_df = (train2_df - train2_df.min(axis = 0)) / (train2_df.max(axis = 0) - train2_df.min(axis = 0))\n",
    "    \n",
    "test2_df = (test2_df - test2_df.min(axis = 0)) / (test2_df.max(axis = 0) - test2_df.min(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 2000\n",
    "mx_dpth = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Код для кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_rgr = xgb.XGBRegressor(colsample_bytree=0.6, learning_rate=0.08, n_estimators=n_est, \\\n",
    "#                            reg_alpha=0.7, reg_lambda=0.3, subsample=0.7, seed=42, max_depth=mx_dpth)\n",
    "# first_end = train2_df.shape[0] * 2 // 3\n",
    "# X = train2_df.iloc[:first_end]\n",
    "# X_t = train2_df.iloc[first_end:]\n",
    "# Y = target_col[:first_end]\n",
    "# Y_t = target_col[first_end:]\n",
    "# xgb_rgr.fit(X, Y)\n",
    "# Y_p = xgb_rgr.predict(X_t)\n",
    "# print(\"MSE:\", sm.mean_squared_error(Y_t, Y_p))\n",
    "\n",
    "\n",
    "# # xgb_rgr = xgb.XGBRegressor(n_estimators=n_est, max_depth=mx_dpth)\n",
    "# # xgb_rgr.fit(train2_df, target_col)\n",
    "# # Y_test = xgb_rgr.predict(test2_df)\n",
    "\n",
    "# # sample_df.head()\n",
    "# steps = 10\n",
    "# d = {}\n",
    "# first_end = train2_df.shape[0] // 2\n",
    "# dist = train2_df.shape[0] // (2 * steps)\n",
    "\n",
    "# parameters = [0.025, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# for i in tqdm(range(steps)):\n",
    "#     X = train2_df.iloc[:first_end + dist * i]\n",
    "#     Y = target_col[:first_end + dist*i]\n",
    "#     X_t = train2_df.iloc[first_end + dist * i : first_end + dist * (i+1)]\n",
    "#     Y_t = target_col[first_end + dist * i : first_end + dist * (i+1)]\n",
    "#     best = 1000000000000000000\n",
    "    \n",
    "#     for k in parameters:\n",
    "#         #print(\"step =\", i, \"alfa =\", k)\n",
    "#         model = Lasso(alpha = k)\n",
    "#         model.fit(X, Y)\n",
    "#         Y_pred = model.predict(X_t)\n",
    "#         num = sm.mean_squared_error(Y_t, Y_pred)\n",
    "#         if num < best:\n",
    "#             best = num\n",
    "#             d[i] = k\n",
    "#             print(\"New Mean Error:\", best)\n",
    "            \n",
    "# d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(ids, preds, output_path='submission.csv'):\n",
    "    subm = pd.DataFrame()\n",
    "    subm['id'] = ids\n",
    "    subm['price'] = preds\n",
    "    subm.to_csv(output_path, index=False)\n",
    "    \n",
    "model = xgb.XGBRegressor(colsample_bytree=0.6, learning_rate=0.08, n_estimators=n_est, \\\n",
    "                           reg_alpha=0.75, reg_lambda=0.45, subsample=0.75, seed=42, max_depth=mx_dpth)\n",
    "model.fit(train2_df, target_col)\n",
    "Y_test = model.predict(test2_df)\n",
    "make_submission(indexes, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv(\"submission.csv\")\n",
    "ans.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <center>Сделано Бронниковым Максимом</center>\n",
    "###### <center>13.12.2019</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
